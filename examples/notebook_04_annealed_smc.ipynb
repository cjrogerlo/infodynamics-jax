{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 4: Uncertainty Quantification with Annealed SMC\n",
    "\n",
    "This notebook demonstrates using Annealed Sequential Monte Carlo (SMC) for full Bayesian inference.\n",
    "\n",
    "**Learning objectives:**\n",
    "- Understand the difference between MAP-II and full Bayesian inference\n",
    "- Learn how Annealed SMC works\n",
    "- Quantify hyperparameter uncertainty\n",
    "- Compare point estimates vs. posterior distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable auto-reload for development\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Fix import path\n",
    "import sys\n",
    "if '..' not in sys.path:\n",
    "    sys.path.insert(0, '..')\n",
    "\n",
    "import os\n",
    "os.environ['JAX_PLATFORM_NAME'] = 'cpu'\n",
    "\n",
    "import jax\n",
    "jax.config.update('jax_platform_name', 'cpu')\n",
    "\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from infodynamics_jax.core import Phi\n",
    "from infodynamics_jax.gp.kernels.params import KernelParams\n",
    "from infodynamics_jax.gp.kernels.rbf import rbf as rbf_kernel\n",
    "from infodynamics_jax.gp.likelihoods import get as get_likelihood\n",
    "from infodynamics_jax.energy import InertialEnergy, InertialCFG\n",
    "from infodynamics_jax.inference.particle import AnnealedSMC, AnnealedSMCCFG\n",
    "from infodynamics_jax.inference.optimisation import TypeII, TypeIICFG\n",
    "from infodynamics_jax.infodynamics import run, RunCFG\n",
    "\n",
    "print(f\"JAX version: {jax.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. MAP-II vs. Full Bayesian Inference\n",
    "\n",
    "### MAP-II (Maximum A Posteriori Type-II)\n",
    "- Finds **single best** hyperparameters: $\\phi^* = \\arg\\max_{\\phi} p(\\phi | y)$\n",
    "- Fast, deterministic\n",
    "- No uncertainty quantification for hyperparameters\n",
    "- Risk of overfitting with small datasets\n",
    "\n",
    "### Full Bayesian (Annealed SMC)\n",
    "- Approximates **entire posterior**: $p(\\phi | y)$\n",
    "- Represents uncertainty via particle cloud\n",
    "- More robust to overfitting\n",
    "- Computational cost: $O(P \\times T)$ where $P$ = particles, $T$ = annealing steps\n",
    "\n",
    "### Annealing Schedule\n",
    "\n",
    "Annealed SMC uses a temperature schedule $\\{\\beta_t\\}_{t=0}^T$ where $0 = \\beta_0 < \\beta_1 < ... < \\beta_T = 1$:\n",
    "\n",
    "$$\\pi_t(\\phi) \\propto p(\\phi) \\cdot p(y | \\phi)^{\\beta_t}$$\n",
    "\n",
    "- $t=0$: Start from prior $p(\\phi)$\n",
    "- $t=T$: End at posterior $p(\\phi | y)$\n",
    "- Intermediate: Tempered distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Generate Regression Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.key(789)\n",
    "\n",
    "# Small dataset to see uncertainty\n",
    "N_train = 30\n",
    "X_train = jnp.linspace(-4, 4, N_train)[:, None]\n",
    "\n",
    "# True function\n",
    "def true_function(x):\n",
    "    return jnp.sin(2 * x[:, 0]) + 0.3 * x[:, 0]\n",
    "\n",
    "f_train = true_function(X_train)\n",
    "\n",
    "# Add noise\n",
    "key, subkey = jax.random.split(key)\n",
    "noise_std = 0.3\n",
    "Y_train = f_train + noise_std * jax.random.normal(subkey, (N_train,))\n",
    "\n",
    "# Test set\n",
    "X_test = jnp.linspace(-5, 5, 100)[:, None]\n",
    "f_test = true_function(X_test)\n",
    "\n",
    "print(f\"Training set: {N_train} points\")\n",
    "print(f\"True noise std: {noise_std}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize data\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.scatter(X_train[:, 0], Y_train, c='red', s=50, alpha=0.6, label='Training data')\n",
    "plt.plot(X_test[:, 0], f_test, 'g-', linewidth=2, label='True function')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.title('Small Dataset for Bayesian Inference')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Baseline: MAP-II Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize\n",
    "kernel_params = KernelParams(lengthscale=jnp.array(1.0), variance=jnp.array(1.0))\n",
    "M = 10\n",
    "Z = jnp.linspace(X_train.min(), X_train.max(), M)[:, None]\n",
    "\n",
    "phi_init = Phi(\n",
    "    kernel_params=kernel_params,\n",
    "    Z=Z,\n",
    "    likelihood_params={\"noise_var\": jnp.array(0.1)},\n",
    "    jitter=1e-5,\n",
    ")\n",
    "\n",
    "# Energy\n",
    "gaussian_likelihood = get_likelihood(\"gaussian\")\n",
    "inertial_cfg = InertialCFG(estimator=\"gh\", gh_n=20, inner_steps=0)\n",
    "inertial_energy = InertialEnergy(\n",
    "    kernel_fn=rbf_kernel,\n",
    "    likelihood=gaussian_likelihood,\n",
    "    cfg=inertial_cfg,\n",
    ")\n",
    "\n",
    "# Run MAP-II\n",
    "typeii_cfg = TypeIICFG(steps=100, lr=1e-2, optimizer=\"adam\", jit=True, constrain_params=True)\n",
    "method = TypeII(cfg=typeii_cfg)\n",
    "\n",
    "key, subkey = jax.random.split(key)\n",
    "out_map = run(\n",
    "    key=subkey,\n",
    "    method=method,\n",
    "    energy=inertial_energy,\n",
    "    phi_init=phi_init,\n",
    "    energy_args=(X_train, Y_train),\n",
    "    cfg=RunCFG(jit=True),\n",
    ")\n",
    "\n",
    "phi_map = out_map.result.phi\n",
    "\n",
    "print(\"MAP-II Results:\")\n",
    "print(f\"  Lengthscale: {float(phi_map.kernel_params.lengthscale):.3f}\")\n",
    "print(f\"  Variance: {float(phi_map.kernel_params.variance):.3f}\")\n",
    "print(f\"  Noise var: {float(phi_map.likelihood_params['noise_var']):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Run Annealed SMC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_particles_fn(key, n_particles: int):\n",
    "    \"\"\"\n",
    "    Initialize particles from prior.\n",
    "    Add noise around initial values to explore the space.\n",
    "    \"\"\"\n",
    "    keys = jax.random.split(key, n_particles)\n",
    "    \n",
    "    def init_one(key_i):\n",
    "        # Sample from prior with some spread\n",
    "        key_l, key_v, key_z, key_n = jax.random.split(key_i, 4)\n",
    "        \n",
    "        # Lengthscale: log-normal around 1.0\n",
    "        lengthscale = jnp.exp(jax.random.normal(key_l, ()) * 0.5)\n",
    "        \n",
    "        # Variance: log-normal around 1.0\n",
    "        variance = jnp.exp(jax.random.normal(key_v, ()) * 0.5)\n",
    "        \n",
    "        # Inducing points: slight perturbation\n",
    "        Z_noisy = phi_init.Z + jax.random.normal(key_z, phi_init.Z.shape) * 0.2\n",
    "        \n",
    "        # Noise variance: log-normal around 0.1\n",
    "        noise_var = jnp.exp(jnp.log(0.1) + jax.random.normal(key_n, ()) * 0.5)\n",
    "        \n",
    "        # Constrain to positive\n",
    "        lengthscale = jnp.maximum(lengthscale, 0.1)\n",
    "        variance = jnp.maximum(variance, 0.1)\n",
    "        noise_var = jnp.maximum(noise_var, 0.01)\n",
    "        \n",
    "        return Phi(\n",
    "            kernel_params=KernelParams(lengthscale=lengthscale, variance=variance),\n",
    "            Z=Z_noisy,\n",
    "            likelihood_params={\"noise_var\": noise_var},\n",
    "            jitter=phi_init.jitter,\n",
    "        )\n",
    "    \n",
    "    particles = jax.vmap(init_one)(keys)\n",
    "    return particles\n",
    "\n",
    "print(\"Particle initialization function created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure Annealed SMC\n",
    "smc_cfg = AnnealedSMCCFG(\n",
    "    n_particles=64,           # Number of particles\n",
    "    n_steps=20,               # Number of annealing steps\n",
    "    ess_threshold=0.5,        # Resample when ESS < 0.5 * n_particles\n",
    "    rejuvenation=\"hmc\",       # Use HMC for particle rejuvenation\n",
    "    rejuvenation_steps=2,     # HMC steps per rejuvenation\n",
    "    jit=True,                 # Enable JIT\n",
    ")\n",
    "\n",
    "method_smc = AnnealedSMC(cfg=smc_cfg)\n",
    "\n",
    "print(f\"Annealed SMC Configuration:\")\n",
    "print(f\"  Particles: {smc_cfg.n_particles}\")\n",
    "print(f\"  Annealing steps: {smc_cfg.n_steps}\")\n",
    "print(f\"  Rejuvenation: {smc_cfg.rejuvenation} ({smc_cfg.rejuvenation_steps} steps)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Annealed SMC\n",
    "print(\"Running Annealed SMC...\")\n",
    "print(\"This may take a minute...\")\n",
    "\n",
    "key, subkey = jax.random.split(key)\n",
    "result_smc = method_smc.run(\n",
    "    energy=inertial_energy,\n",
    "    init_particles_fn=init_particles_fn,\n",
    "    key=subkey,\n",
    "    energy_args=(X_train, Y_train),\n",
    ")\n",
    "\n",
    "particles = result_smc.particles\n",
    "logw = result_smc.logw\n",
    "ess_trace = result_smc.ess_trace\n",
    "logZ_est = result_smc.logZ_est\n",
    "\n",
    "print(\"\\nAnnealed SMC Results:\")\n",
    "print(f\"  Final ESS: {ess_trace[-1]:.1f} / {smc_cfg.n_particles}\")\n",
    "print(f\"  logZ estimate: {logZ_est:.2f}\")\n",
    "print(f\"  Log weight range: [{logw.min():.2f}, {logw.max():.2f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Analyze Posterior Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract particle values\n",
    "lengthscales = np.array(particles.kernel_params.lengthscale)\n",
    "variances = np.array(particles.kernel_params.variance)\n",
    "noise_vars = np.array(particles.likelihood_params[\"noise_var\"])\n",
    "\n",
    "# Normalize weights\n",
    "weights = np.exp(logw - logw.max())\n",
    "weights = weights / weights.sum()\n",
    "\n",
    "# Weighted statistics\n",
    "lengthscale_mean = float(np.sum(weights * lengthscales))\n",
    "lengthscale_std = float(np.sqrt(np.sum(weights * (lengthscales - lengthscale_mean)**2)))\n",
    "\n",
    "variance_mean = float(np.sum(weights * variances))\n",
    "variance_std = float(np.sqrt(np.sum(weights * (variances - variance_mean)**2)))\n",
    "\n",
    "noise_var_mean = float(np.sum(weights * noise_vars))\n",
    "noise_var_std = float(np.sqrt(np.sum(weights * (noise_vars - noise_var_mean)**2)))\n",
    "\n",
    "print(\"Posterior Statistics (Weighted):\")\n",
    "print(f\"\\nLengthscale:\")\n",
    "print(f\"  Mean: {lengthscale_mean:.3f} ± {lengthscale_std:.3f}\")\n",
    "print(f\"  MAP:  {float(phi_map.kernel_params.lengthscale):.3f}\")\n",
    "\n",
    "print(f\"\\nVariance:\")\n",
    "print(f\"  Mean: {variance_mean:.3f} ± {variance_std:.3f}\")\n",
    "print(f\"  MAP:  {float(phi_map.kernel_params.variance):.3f}\")\n",
    "\n",
    "print(f\"\\nNoise Variance:\")\n",
    "print(f\"  Mean: {noise_var_mean:.3f} ± {noise_var_std:.3f}\")\n",
    "print(f\"  MAP:  {float(phi_map.likelihood_params['noise_var']):.3f}\")\n",
    "print(f\"  True: {noise_std**2:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualize Posterior Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "# ESS trace\n",
    "ax = axes[0, 0]\n",
    "ax.plot(ess_trace, 'b-', linewidth=2)\n",
    "ax.axhline(y=smc_cfg.ess_threshold * smc_cfg.n_particles, \n",
    "          color='red', linestyle='--', label=f'Threshold ({smc_cfg.ess_threshold * smc_cfg.n_particles:.0f})')\n",
    "ax.set_xlabel('Annealing Step')\n",
    "ax.set_ylabel('ESS')\n",
    "ax.set_title('Effective Sample Size')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Lengthscale distribution\n",
    "ax = axes[0, 1]\n",
    "ax.hist(lengthscales, bins=30, weights=weights, alpha=0.7, edgecolor='black')\n",
    "ax.axvline(x=lengthscale_mean, color='blue', linestyle='-', linewidth=2, label=f'Mean: {lengthscale_mean:.2f}')\n",
    "ax.axvline(x=float(phi_map.kernel_params.lengthscale), color='red', linestyle='--', linewidth=2, label=f'MAP: {float(phi_map.kernel_params.lengthscale):.2f}')\n",
    "ax.set_xlabel('Lengthscale')\n",
    "ax.set_ylabel('Posterior Density (Weighted)')\n",
    "ax.set_title('Lengthscale Posterior')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Variance distribution\n",
    "ax = axes[0, 2]\n",
    "ax.hist(variances, bins=30, weights=weights, alpha=0.7, edgecolor='black')\n",
    "ax.axvline(x=variance_mean, color='blue', linestyle='-', linewidth=2, label=f'Mean: {variance_mean:.2f}')\n",
    "ax.axvline(x=float(phi_map.kernel_params.variance), color='red', linestyle='--', linewidth=2, label=f'MAP: {float(phi_map.kernel_params.variance):.2f}')\n",
    "ax.set_xlabel('Variance')\n",
    "ax.set_ylabel('Posterior Density (Weighted)')\n",
    "ax.set_title('Variance Posterior')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Noise variance distribution\n",
    "ax = axes[1, 0]\n",
    "ax.hist(noise_vars, bins=30, weights=weights, alpha=0.7, edgecolor='black')\n",
    "ax.axvline(x=noise_var_mean, color='blue', linestyle='-', linewidth=2, label=f'Mean: {noise_var_mean:.2f}')\n",
    "ax.axvline(x=float(phi_map.likelihood_params['noise_var']), color='red', linestyle='--', linewidth=2, label=f'MAP: {float(phi_map.likelihood_params[\"noise_var\"]):.2f}')\n",
    "ax.axvline(x=noise_std**2, color='green', linestyle=':', linewidth=2, label=f'True: {noise_std**2:.2f}')\n",
    "ax.set_xlabel('Noise Variance')\n",
    "ax.set_ylabel('Posterior Density (Weighted)')\n",
    "ax.set_title('Noise Variance Posterior')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Joint distribution: lengthscale vs variance\n",
    "ax = axes[1, 1]\n",
    "scatter = ax.scatter(lengthscales, variances, c=weights, s=100, alpha=0.6, cmap='viridis', edgecolors='black')\n",
    "ax.scatter(float(phi_map.kernel_params.lengthscale), float(phi_map.kernel_params.variance), \n",
    "          c='red', s=200, marker='*', edgecolors='black', linewidths=2, label='MAP', zorder=10)\n",
    "ax.set_xlabel('Lengthscale')\n",
    "ax.set_ylabel('Variance')\n",
    "ax.set_title('Joint Posterior (Lengthscale vs Variance)')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.colorbar(scatter, ax=ax, label='Weight')\n",
    "\n",
    "# Particle weights\n",
    "ax = axes[1, 2]\n",
    "ax.bar(range(len(weights)), np.sort(weights)[::-1], alpha=0.7, edgecolor='black')\n",
    "ax.set_xlabel('Particle Index (sorted)')\n",
    "ax.set_ylabel('Normalized Weight')\n",
    "ax.set_title('Particle Weights Distribution')\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we demonstrated full Bayesian inference with Annealed SMC:\n",
    "\n",
    "### Key Findings\n",
    "\n",
    "1. **Posterior Uncertainty**: SMC provides full posterior distributions, not just point estimates\n",
    "2. **MAP vs. Bayesian Mean**: MAP estimates can differ from posterior means\n",
    "3. **Hyperparameter Correlations**: Joint distributions reveal parameter dependencies\n",
    "4. **Uncertainty Quantification**: Standard deviations quantify estimation uncertainty\n",
    "\n",
    "### When to Use Annealed SMC\n",
    "\n",
    "**Use Annealed SMC when:**\n",
    "- Small datasets (high parameter uncertainty)\n",
    "- Need robust inference (avoid overfitting)\n",
    "- Want to quantify hyperparameter uncertainty\n",
    "- Model selection (via marginal likelihood estimate)\n",
    "\n",
    "**Use MAP-II when:**\n",
    "- Large datasets (posterior concentrates)\n",
    "- Speed is critical\n",
    "- Point estimates are sufficient\n",
    "- Production deployment\n",
    "\n",
    "### Annealing Best Practices\n",
    "\n",
    "1. **Number of particles**: Start with 50-100, increase if ESS drops too much\n",
    "2. **Annealing steps**: 10-20 usually sufficient\n",
    "3. **Rejuvenation**: HMC with 1-5 steps maintains diversity\n",
    "4. **ESS threshold**: 0.5 is standard, lower = more resampling\n",
    "\n",
    "### Computational Cost\n",
    "\n",
    "- **MAP-II**: $O(N \\times I)$ where $I$ = iterations (typically 100-500)\n",
    "- **Annealed SMC**: $O(P \\times T \\times C)$ where:\n",
    "  - $P$ = particles (50-200)\n",
    "  - $T$ = annealing steps (10-20)\n",
    "  - $C$ = cost per particle (similar to MAP-II iteration)\n",
    "\n",
    "**Rule of thumb**: SMC is 10-50× slower than MAP-II, but provides full uncertainty quantification."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
