{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 3: GP Classification with Non-Conjugate Likelihoods\n",
    "\n",
    "This notebook demonstrates Gaussian Process Classification using non-conjugate likelihoods.\n",
    "\n",
    "**Learning objectives:**\n",
    "- Understand non-conjugate likelihoods (Bernoulli, Poisson)\n",
    "- Learn about Gauss-Hermite and Monte Carlo estimators\n",
    "- Perform binary and multi-class classification\n",
    "- Visualize decision boundaries and uncertainty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable auto-reload for development\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Fix import path\n",
    "import sys\n",
    "if '..' not in sys.path:\n",
    "    sys.path.insert(0, '..')\n",
    "\n",
    "import os\n",
    "os.environ['JAX_PLATFORM_NAME'] = 'cpu'\n",
    "\n",
    "import jax\n",
    "jax.config.update('jax_platform_name', 'cpu')\n",
    "\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "from infodynamics_jax.core import Phi\n",
    "from infodynamics_jax.gp.kernels.params import KernelParams\n",
    "from infodynamics_jax.gp.kernels.rbf import rbf as rbf_kernel\n",
    "from infodynamics_jax.gp.likelihoods import get as get_likelihood\n",
    "from infodynamics_jax.energy import InertialEnergy, InertialCFG\n",
    "from infodynamics_jax.inference.optimisation import TypeII, TypeIICFG\n",
    "from infodynamics_jax.infodynamics import run, RunCFG\n",
    "from infodynamics_jax.gp.ansatz.state import VariationalState\n",
    "from infodynamics_jax.gp.ansatz.expected import qfi_from_qu_full\n",
    "\n",
    "print(f\"JAX version: {jax.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Understanding Non-Conjugate Likelihoods\n",
    "\n",
    "### Conjugate vs. Non-Conjugate\n",
    "\n",
    "**Conjugate (Gaussian likelihood)**:\n",
    "- Posterior has closed-form solution\n",
    "- Fast, exact inference\n",
    "- Limited to regression\n",
    "\n",
    "**Non-Conjugate (Bernoulli, Poisson, etc.)**:\n",
    "- No closed-form posterior\n",
    "- Requires approximation methods:\n",
    "  - **Gauss-Hermite (GH)**: Deterministic quadrature\n",
    "  - **Monte Carlo (MC)**: Stochastic sampling\n",
    "- Enables classification, count data, etc.\n",
    "\n",
    "### Available Non-Conjugate Likelihoods\n",
    "\n",
    "1. **Bernoulli**: Binary classification\n",
    "   - $p(y=1|f) = \\sigma(f)$ where $\\sigma$ is sigmoid\n",
    "   \n",
    "2. **Poisson**: Count data\n",
    "   - $p(y|f) = \\text{Poisson}(y | \\lambda = \\exp(f))$\n",
    "   \n",
    "3. **Negative Binomial**: Over-dispersed counts\n",
    "   - Generalizes Poisson\n",
    "   \n",
    "4. **Ordinal**: Ordered categorical data\n",
    "   - For ratings, severity levels, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Generate Binary Classification Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.key(456)\n",
    "\n",
    "# Generate 2D data\n",
    "N_train = 100\n",
    "\n",
    "# Class 0: cluster around (-2, -2)\n",
    "key, subkey = jax.random.split(key)\n",
    "X_class0 = jax.random.normal(subkey, (N_train // 2, 2)) * 0.8 + jnp.array([-2.0, -2.0])\n",
    "\n",
    "# Class 1: cluster around (2, 2)\n",
    "key, subkey = jax.random.split(key)\n",
    "X_class1 = jax.random.normal(subkey, (N_train // 2, 2)) * 0.8 + jnp.array([2.0, 2.0])\n",
    "\n",
    "# Combine\n",
    "X_train = jnp.vstack([X_class0, X_class1])\n",
    "Y_train = jnp.concatenate([\n",
    "    jnp.zeros(N_train // 2),\n",
    "    jnp.ones(N_train // 2)\n",
    "])\n",
    "\n",
    "# Shuffle\n",
    "key, subkey = jax.random.split(key)\n",
    "perm = jax.random.permutation(subkey, N_train)\n",
    "X_train = X_train[perm]\n",
    "Y_train = Y_train[perm]\n",
    "\n",
    "print(f\"Training set: {N_train} points\")\n",
    "print(f\"X shape: {X_train.shape}, Y shape: {Y_train.shape}\")\n",
    "print(f\"Class 0: {jnp.sum(Y_train == 0)} points\")\n",
    "print(f\"Class 1: {jnp.sum(Y_train == 1)} points\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training data\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X_train[Y_train == 0, 0], X_train[Y_train == 0, 1], \n",
    "           c='blue', s=50, alpha=0.6, label='Class 0', edgecolors='k')\n",
    "plt.scatter(X_train[Y_train == 1, 0], X_train[Y_train == 1, 1], \n",
    "           c='red', s=50, alpha=0.6, label='Class 1', edgecolors='k')\n",
    "plt.xlabel('X1', fontsize=12)\n",
    "plt.ylabel('X2', fontsize=12)\n",
    "plt.title('Binary Classification Data', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train GP Classifier with Bernoulli Likelihood\n",
    "\n",
    "We use Gauss-Hermite quadrature to handle the non-conjugate Bernoulli likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Bernoulli likelihood\n",
    "bernoulli_likelihood = get_likelihood(\"bernoulli\")\n",
    "\n",
    "# Initialize kernel parameters\n",
    "kernel_params = KernelParams(\n",
    "    lengthscale=jnp.array(1.0),\n",
    "    variance=jnp.array(1.0),\n",
    ")\n",
    "\n",
    "# Create inducing points (grid)\n",
    "M = 20\n",
    "Z_x1 = jnp.linspace(X_train[:, 0].min(), X_train[:, 0].max(), int(jnp.sqrt(M)))\n",
    "Z_x2 = jnp.linspace(X_train[:, 1].min(), X_train[:, 1].max(), int(jnp.sqrt(M)))\n",
    "Z_grid = jnp.stack(jnp.meshgrid(Z_x1, Z_x2), axis=-1).reshape(-1, 2)\n",
    "Z = Z_grid[:M]  # Take first M points\n",
    "\n",
    "# Create Phi (no noise variance for Bernoulli)\n",
    "phi_init = Phi(\n",
    "    kernel_params=kernel_params,\n",
    "    Z=Z,\n",
    "    likelihood_params={},  # Bernoulli has no extra parameters\n",
    "    jitter=1e-5,\n",
    ")\n",
    "\n",
    "print(f\"Number of inducing points: {len(Z)}\")\n",
    "print(f\"Kernel lengthscale: {phi_init.kernel_params.lengthscale}\")\n",
    "print(f\"Kernel variance: {phi_init.kernel_params.variance}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create InertialEnergy with Gauss-Hermite estimator\n",
    "inertial_cfg = InertialCFG(\n",
    "    estimator=\"gh\",  # Gauss-Hermite quadrature\n",
    "    gh_n=20,         # Number of quadrature points\n",
    "    inner_steps=0,   # No inner optimization\n",
    ")\n",
    "\n",
    "inertial_energy = InertialEnergy(\n",
    "    kernel_fn=rbf_kernel,\n",
    "    likelihood=bernoulli_likelihood,\n",
    "    cfg=inertial_cfg,\n",
    ")\n",
    "\n",
    "print(\"InertialEnergy created with Bernoulli likelihood!\")\n",
    "print(f\"Estimator: {inertial_cfg.estimator}\")\n",
    "print(f\"GH quadrature points: {inertial_cfg.gh_n}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure and run TypeII\n",
    "typeii_cfg = TypeIICFG(\n",
    "    steps=150,\n",
    "    lr=1e-2,\n",
    "    optimizer=\"adam\",\n",
    "    jit=True,\n",
    "    constrain_params=True,\n",
    ")\n",
    "\n",
    "method = TypeII(cfg=typeii_cfg)\n",
    "\n",
    "key, subkey = jax.random.split(key)\n",
    "out = run(\n",
    "    key=subkey,\n",
    "    method=method,\n",
    "    energy=inertial_energy,\n",
    "    phi_init=phi_init,\n",
    "    energy_args=(X_train, Y_train),\n",
    "    cfg=RunCFG(jit=True),\n",
    ")\n",
    "\n",
    "phi_opt = out.result.phi\n",
    "energy_trace = out.result.energy_trace\n",
    "\n",
    "print(\"\\nOptimization complete!\")\n",
    "print(f\"Final energy: {energy_trace[-1]:.2f}\")\n",
    "print(f\"Energy reduction: {energy_trace[0] - energy_trace[-1]:.2f}\")\n",
    "print(f\"\\nOptimized hyperparameters:\")\n",
    "print(f\"  Lengthscale: {float(phi_opt.kernel_params.lengthscale):.3f}\")\n",
    "print(f\"  Variance: {float(phi_opt.kernel_params.variance):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot optimization trace\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(energy_trace, 'b-', linewidth=2)\n",
    "plt.xlabel('Iteration', fontsize=12)\n",
    "plt.ylabel('Energy', fontsize=12)\n",
    "plt.title('MAP-II Optimization Trace (Bernoulli Likelihood)', fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Make Predictions and Visualize Decision Boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create grid for prediction\n",
    "resolution = 100\n",
    "x1_min, x1_max = X_train[:, 0].min() - 1, X_train[:, 0].max() + 1\n",
    "x2_min, x2_max = X_train[:, 1].min() - 1, X_train[:, 1].max() + 1\n",
    "xx1, xx2 = jnp.meshgrid(\n",
    "    jnp.linspace(x1_min, x1_max, resolution),\n",
    "    jnp.linspace(x2_min, x2_max, resolution)\n",
    ")\n",
    "X_grid = jnp.c_[xx1.ravel(), xx2.ravel()]\n",
    "\n",
    "print(f\"Grid shape: {X_grid.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute posterior state\n",
    "state = VariationalState.initialise(phi_opt, X_train, Y_train)\n",
    "\n",
    "# Make predictions on grid\n",
    "mu_grid, var_grid = qfi_from_qu_full(\n",
    "    phi_opt, X_grid, rbf_kernel, state.m_u, state.L_u\n",
    ")\n",
    "\n",
    "mu_grid = mu_grid.squeeze()\n",
    "var_grid = var_grid.squeeze()\n",
    "\n",
    "# Convert latent function to probabilities: p(y=1) = sigmoid(f)\n",
    "prob_class1 = jax.nn.sigmoid(mu_grid)\n",
    "prob_class1 = prob_class1.reshape(xx1.shape)\n",
    "\n",
    "print(f\"Predictions computed on {len(X_grid)} grid points\")\n",
    "print(f\"Probability range: [{prob_class1.min():.3f}, {prob_class1.max():.3f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize decision boundary\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Plot 1: Decision boundary\n",
    "ax = axes[0]\n",
    "contour = ax.contourf(xx1, xx2, prob_class1, levels=20, cmap='RdBu_r', alpha=0.8)\n",
    "ax.contour(xx1, xx2, prob_class1, levels=[0.5], colors='black', linewidths=2)\n",
    "ax.scatter(X_train[Y_train == 0, 0], X_train[Y_train == 0, 1], \n",
    "          c='blue', s=50, alpha=0.8, edgecolors='k', label='Class 0')\n",
    "ax.scatter(X_train[Y_train == 1, 0], X_train[Y_train == 1, 1], \n",
    "          c='red', s=50, alpha=0.8, edgecolors='k', label='Class 1')\n",
    "ax.scatter(phi_opt.Z[:, 0], phi_opt.Z[:, 1], \n",
    "          c='green', marker='x', s=100, linewidths=2, label='Inducing points')\n",
    "ax.set_xlabel('X1', fontsize=12)\n",
    "ax.set_ylabel('X2', fontsize=12)\n",
    "ax.set_title('Decision Boundary (p(y=1))', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=10)\n",
    "plt.colorbar(contour, ax=ax, label='P(Class 1)')\n",
    "\n",
    "# Plot 2: Prediction uncertainty\n",
    "ax = axes[1]\n",
    "std_grid = jnp.sqrt(var_grid).reshape(xx1.shape)\n",
    "contour = ax.contourf(xx1, xx2, std_grid, levels=20, cmap='viridis', alpha=0.8)\n",
    "ax.scatter(X_train[Y_train == 0, 0], X_train[Y_train == 0, 1], \n",
    "          c='blue', s=50, alpha=0.5, edgecolors='k')\n",
    "ax.scatter(X_train[Y_train == 1, 0], X_train[Y_train == 1, 1], \n",
    "          c='red', s=50, alpha=0.5, edgecolors='k')\n",
    "ax.set_xlabel('X1', fontsize=12)\n",
    "ax.set_ylabel('X2', fontsize=12)\n",
    "ax.set_title('Prediction Uncertainty (std)', fontsize=14, fontweight='bold')\n",
    "plt.colorbar(contour, ax=ax, label='Std Dev')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluate Classification Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on training set\n",
    "mu_train, var_train = qfi_from_qu_full(\n",
    "    phi_opt, X_train, rbf_kernel, state.m_u, state.L_u\n",
    ")\n",
    "\n",
    "mu_train = mu_train.squeeze()\n",
    "prob_train = jax.nn.sigmoid(mu_train)\n",
    "Y_pred = (prob_train > 0.5).astype(float)\n",
    "\n",
    "# Compute metrics\n",
    "accuracy = float(jnp.mean(Y_pred == Y_train))\n",
    "true_positives = float(jnp.sum((Y_pred == 1) & (Y_train == 1)))\n",
    "false_positives = float(jnp.sum((Y_pred == 1) & (Y_train == 0)))\n",
    "true_negatives = float(jnp.sum((Y_pred == 0) & (Y_train == 0)))\n",
    "false_negatives = float(jnp.sum((Y_pred == 0) & (Y_train == 1)))\n",
    "\n",
    "precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0\n",
    "recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0\n",
    "f1_score = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "print(\"Classification Metrics (Training Set):\")\n",
    "print(f\"  Accuracy:  {accuracy:.3f}\")\n",
    "print(f\"  Precision: {precision:.3f}\")\n",
    "print(f\"  Recall:    {recall:.3f}\")\n",
    "print(f\"  F1 Score:  {f1_score:.3f}\")\n",
    "print(f\"\\nConfusion Matrix:\")\n",
    "print(f\"  TP: {int(true_positives):<3} FP: {int(false_positives):<3}\")\n",
    "print(f\"  FN: {int(false_negatives):<3} TN: {int(true_negatives):<3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Comparing Gauss-Hermite vs. Monte Carlo Estimators\n",
    "\n",
    "Let's compare the two estimators for non-conjugate likelihoods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train with Monte Carlo estimator\n",
    "inertial_cfg_mc = InertialCFG(\n",
    "    estimator=\"mc\",      # Monte Carlo\n",
    "    n_mc_samples=100,    # Number of MC samples\n",
    "    inner_steps=0,\n",
    ")\n",
    "\n",
    "inertial_energy_mc = InertialEnergy(\n",
    "    kernel_fn=rbf_kernel,\n",
    "    likelihood=bernoulli_likelihood,\n",
    "    cfg=inertial_cfg_mc,\n",
    ")\n",
    "\n",
    "key, subkey = jax.random.split(key)\n",
    "out_mc = run(\n",
    "    key=subkey,\n",
    "    method=method,\n",
    "    energy=inertial_energy_mc,\n",
    "    phi_init=phi_init,\n",
    "    energy_args=(X_train, Y_train),\n",
    "    cfg=RunCFG(jit=True),\n",
    ")\n",
    "\n",
    "phi_opt_mc = out_mc.result.phi\n",
    "energy_trace_mc = out_mc.result.energy_trace\n",
    "\n",
    "print(\"\\nMonte Carlo Estimator Results:\")\n",
    "print(f\"Final energy: {energy_trace_mc[-1]:.2f}\")\n",
    "print(f\"Optimized lengthscale: {float(phi_opt_mc.kernel_params.lengthscale):.3f}\")\n",
    "print(f\"Optimized variance: {float(phi_opt_mc.kernel_params.variance):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare optimization traces\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(energy_trace, 'b-', linewidth=2, label='Gauss-Hermite', alpha=0.8)\n",
    "plt.plot(energy_trace_mc, 'r--', linewidth=2, label='Monte Carlo', alpha=0.8)\n",
    "plt.xlabel('Iteration', fontsize=12)\n",
    "plt.ylabel('Energy', fontsize=12)\n",
    "plt.title('Estimator Comparison', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nComparison:\")\n",
    "print(f\"GH final energy:  {energy_trace[-1]:.2f}\")\n",
    "print(f\"MC final energy:  {energy_trace_mc[-1]:.2f}\")\n",
    "print(f\"\\nNote: MC is stochastic, so the trace is noisy.\")\n",
    "print(f\"      GH is deterministic and generally more stable.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we demonstrated GP classification with non-conjugate likelihoods:\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "1. **Non-Conjugate Likelihoods**: Bernoulli for binary classification\n",
    "2. **Approximation Methods**:\n",
    "   - **Gauss-Hermite**: Deterministic, stable, good for low-dimensional integrals\n",
    "   - **Monte Carlo**: Stochastic, scales to high dimensions, noisier gradients\n",
    "\n",
    "3. **GP Classification**:\n",
    "   - Latent function $f(x)$ modeled by GP\n",
    "   - Probability: $p(y=1|x) = \\sigma(f(x))$\n",
    "   - Natural uncertainty quantification\n",
    "\n",
    "### When to Use Each Estimator\n",
    "\n",
    "| Estimator | Pros | Cons | Best For |\n",
    "|-----------|------|------|----------|\n",
    "| **Gauss-Hermite** | Deterministic, stable | Limited to 1D integrals | Standard classification |\n",
    "| **Monte Carlo** | Scales to high dims | Noisy, slower convergence | Multi-output, complex models |\n",
    "\n",
    "### Extensions\n",
    "\n",
    "- **Multi-class**: Use softmax likelihood\n",
    "- **Count data**: Poisson or Negative Binomial\n",
    "- **Ordinal**: Ordered categories (ratings, severity)\n",
    "- **Robust**: Student-t for outlier resistance"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
