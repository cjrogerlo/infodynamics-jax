{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# %% [markdown]\n",
        "# # Particle Feynman Benchmark: Expanded Synthetic Library\n",
        "# \n",
        "# This notebook demonstrates the performance of ML-II, MAP-II, and SMC-VFE on a diverse library of synthetic functions, ranging from smooth multiscale patterns to discontinuous waves and spiky signals."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import time\n",
        "from pathlib import Path\n",
        "\n",
        "# Force CPU execution for stability with JAX Metal on macOS\n",
        "os.environ[\"JAX_PLATFORM_NAME\"] = \"cpu\"\n",
        "os.environ[\"JAX_PLATFORMS\"] = \"cpu\"\n",
        "os.environ[\"JAX_ENABLE_X64\"] = \"True\"\n",
        "\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib\n",
        "\n",
        "# Ensure repository root is in path\n",
        "cwd = Path(os.getcwd())\n",
        "repo_root = cwd\n",
        "while repo_root.parent != repo_root:\n",
        "    if (repo_root / 'infodynamics_jax').exists():\n",
        "        break\n",
        "    repo_root = repo_root.parent\n",
        "if str(repo_root) not in sys.path:\n",
        "    sys.path.insert(0, str(repo_root))\n",
        "\n",
        "from infodynamics_jax.core import Phi\n",
        "from infodynamics_jax.gp.kernels import rbf\n",
        "from infodynamics_jax.gp.kernels.params import KernelParams\n",
        "from infodynamics_jax.gp.likelihoods import get as get_likelihood\n",
        "from infodynamics_jax.gp.predict import predict_typeii\n",
        "from infodynamics_jax.gp.sparsify import fitc_log_evidence\n",
        "from infodynamics_jax.inference.optimisation import TypeII, TypeIICFG\n",
        "from infodynamics_jax.inference.optimisation.vfe import make_vfe_objective\n",
        "from infodynamics_jax.infodynamics import make_hyperprior\n",
        "from utils import synthetic, compute_metrics, setup_plot_style, COLORS, plot_with_uncertainty\n",
        "from utils.smc_array_only import annealed_smc_array\n",
        "\n",
        "# Initialize plotting style\n",
        "setup_plot_style()\n",
        "matplotlib.use('module://matplotlib_inline.backend_inline')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CFG:\n",
        "    N_train = 120\n",
        "    N_test = 60\n",
        "    noise_std = 0.2\n",
        "    domain = (-2.5, 2.5)\n",
        "    M = 20\n",
        "    n_particles = 64\n",
        "    n_steps = 24\n",
        "    ess_threshold = 0.6\n",
        "    rejuvenation_steps = 2\n",
        "    step_size = 0.02\n",
        "    n_leapfrog = 8\n",
        "    typeii_steps = 300\n",
        "    typeii_lr = 1e-2\n",
        "\n",
        "cfg = CFG()\n",
        "\n",
        "# %% [markdown]\n",
        "# ## Dataset Gallery\n",
        "# Let's visualize the newly added synthetic functions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "all_functions = synthetic._functions.keys()\n",
        "# Filter out some legacy ones for the gallery\n",
        "gallery_functions = [f for f in all_functions if synthetic.get(f)[3] != 'periodic' and synthetic.get(f)[3] != 'linear' and synthetic.get(f)[3] != 'polynomial' and synthetic.get(f)[3] != 'smooth']\n",
        "\n",
        "cols = 4\n",
        "rows = int(np.ceil(len(gallery_functions) / cols))\n",
        "plt.figure(figsize=(20, rows * 4))\n",
        "\n",
        "x_plot = jnp.linspace(cfg.domain[0], cfg.domain[1], 500)\n",
        "for i, name in enumerate(gallery_functions):\n",
        "    fn, title, _, cat = synthetic.get(name)\n",
        "    plt.subplot(rows, cols, i + 1)\n",
        "    plt.plot(x_plot, fn(x_plot), lw=2)\n",
        "    plt.title(f\"{title}\\n({cat})\", fontsize=10)\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# %% [markdown]\n",
        "# ## Inference Utilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def unpack_state(theta, shape_z):\n",
        "    log_ell, log_sf2, log_sn2 = theta[0], theta[1], theta[2]\n",
        "    Z = theta[3:].reshape(shape_z)\n",
        "    return log_ell, log_sf2, log_sn2, Z\n",
        "\n",
        "def get_energy_fn(X, y, hyperprior_fn, shape_z, jitter):\n",
        "    def energy_theta(theta):\n",
        "        log_ell, log_sf2, log_sn2, Z = unpack_state(theta, shape_z)\n",
        "        params = KernelParams(lengthscale=jnp.exp(log_ell), variance=jnp.exp(log_sf2))\n",
        "        noise_var = jnp.exp(log_sn2)\n",
        "        E_fitc = -fitc_log_evidence(kernel_fn=rbf, params=params, X=X, y=y, Z=Z, noise_var=noise_var, jitter=jitter)\n",
        "        phi = Phi(kernel_params=params, Z=Z, likelihood_params={'noise_var': noise_var}, jitter=jitter)\n",
        "        return E_fitc + hyperprior_fn(phi)\n",
        "    return energy_theta\n",
        "\n",
        "def predict_bma(particles, logw, X_star, X_tr, Y_tr, shape_z, jitter):\n",
        "    w = jnp.exp(logw - jax.scipy.special.logsumexp(logw))\n",
        "    mus, vars_ = [], []\n",
        "    for i in range(len(w)):\n",
        "        ll, lv, ln, Z = unpack_state(particles[i], shape_z)\n",
        "        phi_i = Phi(KernelParams(jnp.exp(ll), jnp.exp(lv)), Z, {'noise_var': jnp.exp(ln)}, jitter)\n",
        "        m, v = predict_typeii(phi_i, X_star, X_tr, Y_tr, rbf, residual='fitc')\n",
        "        mus.append(m); vars_.append(v)\n",
        "    mus, vars_ = jnp.stack(mus), jnp.stack(vars_)\n",
        "    mean_bma = (w[:, None] * mus).sum(axis=0)\n",
        "    var_bma = (w[:, None] * (vars_ + mus**2)).sum(axis=0) - mean_bma**2\n",
        "    return mean_bma, jnp.sqrt(jnp.maximum(var_bma, 1e-12)), mus, w\n",
        "\n",
        "# %% [markdown]\n",
        "# ## Unified Benchmark Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_full_benchmark(name, key):\n",
        "    print(f\"\\nEvaluating: {name}...\")\n",
        "    fn, title, _, _ = synthetic.get(name)\n",
        "    key_data, key_smc, key_init = jax.random.split(key, 3)\n",
        "    \n",
        "    # Data\n",
        "    X_all, Y_all, _ = synthetic.sample(name, N=cfg.N_train + cfg.N_test, noise=cfg.noise_std, domain=cfg.domain, key=key_data)\n",
        "    X_all = X_all[:, None]\n",
        "    perm = jax.random.permutation(key_data, X_all.shape[0])\n",
        "    X_tr, Y_tr = X_all[perm[:cfg.N_train]], Y_all[perm[:cfg.N_train]]\n",
        "    X_te, Y_te = X_all[perm[cfg.N_train:]], Y_all[perm[cfg.N_train:]]\n",
        "    X_plot = jnp.linspace(cfg.domain[0], cfg.domain[1], 240)[:, None]\n",
        "    Y_plot = fn(X_plot[:, 0])\n",
        "\n",
        "    # Setup\n",
        "    Z0 = jnp.linspace(cfg.domain[0], cfg.domain[1], cfg.M)[:, None]\n",
        "    phi_init = Phi(KernelParams(jnp.array(1.0), jnp.array(1.0)), Z0, {'noise_var': jnp.array(cfg.noise_std**2)}, 1e-6)\n",
        "    hyperprior_fn = make_hyperprior(kernel_log_lambda=4.0, kernel_fields=[\"lengthscale\", \"variance\"], likelihood_log_lambda=4.0, likelihood_keys=[\"noise_var\"], likelihood_log_mu={'noise_var': jnp.log(cfg.noise_std**2)})\n",
        "\n",
        "    # ML-II\n",
        "    typeii = TypeII(cfg=TypeIICFG(steps=cfg.typeii_steps, lr=cfg.typeii_lr, optimizer='adam', jit=True))\n",
        "    vfe_obj = make_vfe_objective(kernel_fn=rbf, residual='fitc')\n",
        "    res_ml = typeii.run(energy=vfe_obj, phi_init=phi_init, energy_args=(X_tr, Y_tr))\n",
        "    phi_ml = res_ml.phi\n",
        "\n",
        "    # MAP-II\n",
        "    def map_ii_obj(phi, X, y): return vfe_obj(phi, X, y) + hyperprior_fn(phi)\n",
        "    res_map = typeii.run(energy=map_ii_obj, phi_init=phi_init, energy_args=(X_tr, Y_tr))\n",
        "    phi_map = res_map.phi\n",
        "\n",
        "    # SMC\n",
        "    energy_fn = get_energy_fn(X_tr, Y_tr, hyperprior_fn, Z0.shape, phi_init.jitter)\n",
        "    def init_particles(k, n):\n",
        "        kl, kv, kn, kz = jax.random.split(k, 4)\n",
        "        log_l = jnp.log(phi_init.kernel_params.lengthscale) + jax.random.normal(kl, (n,)) * 0.5\n",
        "        log_v = jnp.log(phi_init.kernel_params.variance) + jax.random.normal(kv, (n,)) * 0.5\n",
        "        log_n = jnp.log(phi_init.likelihood_params['noise_var']) + jax.random.normal(kn, (n,)) * 0.5\n",
        "        Z_noisy = Z0[None] + 0.2 * jax.random.normal(kz, (n, *Z0.shape))\n",
        "        return jnp.concatenate([log_l[:,None], log_v[:,None], log_n[:,None], Z_noisy.reshape(n, -1)], axis=1)\n",
        "\n",
        "    smc_res = annealed_smc_array(key=key_smc, init_particles=init_particles(key_init, cfg.n_particles), energy_fn=energy_fn, n_steps=cfg.n_steps, ess_threshold=cfg.ess_threshold, step_size=cfg.step_size, n_leapfrog=cfg.n_leapfrog, rejuvenation_steps=cfg.rejuvenation_steps)\n",
        "\n",
        "    # Predictions\n",
        "    ml_mean, ml_var = predict_typeii(phi_ml, X_plot, X_tr, Y_tr, rbf, residual='fitc')\n",
        "    map_mean, map_var = predict_typeii(phi_map, X_plot, X_tr, Y_tr, rbf, residual='fitc')\n",
        "    smc_mean, smc_std, smc_curves, smc_weights = predict_bma(smc_res['particles'], smc_res['logw'], X_plot, X_tr, Y_tr, Z0.shape, phi_init.jitter)\n",
        "\n",
        "    # Comparison Plot\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.scatter(X_tr, Y_tr, s=10, alpha=0.3, color=\"C7\", label=\"Train\")\n",
        "    plt.plot(X_plot, Y_plot, \"k--\", alpha=0.5, label=\"True f\")\n",
        "    plt.plot(X_plot, ml_mean, color=\"C1\", lw=2, label=\"ML-II\")\n",
        "    plt.plot(X_plot, map_mean, color=\"C2\", lw=2, label=\"MAP-II\")\n",
        "    plt.plot(X_plot, smc_mean, color=\"C0\", lw=2, label=\"SMC-VFE\")\n",
        "    plt.fill_between(X_plot.flatten(), smc_mean - 2*smc_std, smc_mean + 2*smc_std, alpha=0.1, color=\"C0\")\n",
        "    \n",
        "    # Inducing points markers\n",
        "    smc_Z_mean = (smc_weights[:, None, None] * smc_res['particles'][:, 3:].reshape(-1, *Z0.shape)).sum(axis=0)\n",
        "    for z in phi_ml.Z.flatten(): plt.axvline(float(z), color=\"C1\", ls=\":\", alpha=0.2)\n",
        "    for z in phi_map.Z.flatten(): plt.axvline(float(z), color=\"C2\", ls=\"--\", alpha=0.2)\n",
        "    for z in smc_Z_mean.flatten(): plt.axvline(float(z), color=\"C0\", ls=\"-\", alpha=0.1)\n",
        "\n",
        "    plt.title(f\"{name}: Methods Comparison\")\n",
        "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "    plt.tight_layout(); plt.show()\n",
        "\n",
        "    # Metrics\n",
        "    m_te_smc, s_te_smc, _, _ = predict_bma(smc_res['particles'], smc_res['logw'], X_te, X_tr, Y_tr, Z0.shape, phi_init.jitter)\n",
        "    m_te_ml, v_te_ml = predict_typeii(phi_ml, X_te, X_tr, Y_tr, rbf, residual='fitc')\n",
        "    m_te_map, v_te_map = predict_typeii(phi_map, X_te, X_tr, Y_tr, rbf, residual='fitc')\n",
        "    \n",
        "    return {\n",
        "        'ML-II': compute_metrics(Y_te, m_te_ml, jnp.sqrt(v_te_ml)),\n",
        "        'MAP-II': compute_metrics(Y_te, m_te_map, jnp.sqrt(v_te_map)),\n",
        "        'SMC-VFE': compute_metrics(Y_te, m_te_smc, s_te_smc)\n",
        "    }\n",
        "\n",
        "# %% [markdown]\n",
        "# ## Run Benchmark on Selected Functions\n",
        "# We select a representative set of functions to benchmark."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "benchmark_set = ['nonstationary_frequency', 'piecewise_kink', 'step_local_variation', 'spike_train']\n",
        "results = {}\n",
        "key = jax.random.key(123)\n",
        "\n",
        "for name in benchmark_set:\n",
        "    key, subkey = jax.random.split(key)\n",
        "    results[name] = run_full_benchmark(name, subkey)\n",
        "\n",
        "# %% [markdown]\n",
        "# ## Final Summary Table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(f\"{'Dataset':<25} | {'Method':<10} | {'RMSE':<8} | {'NLPD':<8}\")\n",
        "print(\"-\" * 60)\n",
        "for ds, res in results.items():\n",
        "    for method in ['ML-II', 'MAP-II', 'SMC-VFE']:\n",
        "        m = res[method]\n",
        "        print(f\"{ds:<25} | {method:<10} | {m['rmse']:<8.4f} | {m['nlpd']:<8.4f}\")\n",
        "print(\"=\"*60)\n"
      ]
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
